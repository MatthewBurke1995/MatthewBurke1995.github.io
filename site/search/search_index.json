{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Matt's thoughts on things Hi! Thanks for checking out this page. I'm Matt Burke and I'm currently a data scientist located between Seoul and Sydney. I graduated with a BSc in Physics, but discovered I was more interested in software during my studies. In my spare time I like to read non-fiction, go hiking, be disappointed by Tottenham and watch the latest Korean thriller movies. I have used R, Python, Javascript/Node, Java and C# in professional contexts while working at the Reserve Bank of Australia (Data Analyst in a central bank), IBM (Software Engineer in search and web development domains) and Wavebridge (Quantative Researcher in a hedge fund). In terms of human languages I am fluent in Korean.","title":"Matt's thoughts on things"},{"location":"#matts-thoughts-on-things","text":"Hi! Thanks for checking out this page. I'm Matt Burke and I'm currently a data scientist located between Seoul and Sydney. I graduated with a BSc in Physics, but discovered I was more interested in software during my studies. In my spare time I like to read non-fiction, go hiking, be disappointed by Tottenham and watch the latest Korean thriller movies. I have used R, Python, Javascript/Node, Java and C# in professional contexts while working at the Reserve Bank of Australia (Data Analyst in a central bank), IBM (Software Engineer in search and web development domains) and Wavebridge (Quantative Researcher in a hedge fund). In terms of human languages I am fluent in Korean.","title":"Matt's thoughts on things"},{"location":"ABC%20Radio/","text":"ABC Radio I wrote a python library for searching and parsing through the historical catalgoue of songs played on ABC (Australian Broadcasting Corporation) radio channels. This is especially timely since TripleJ will be releasing it's \"Hottest 100\" playlist for 2022 in a few days. I'm particularly interested in the TripleJ data due to it aligning with my music preferences. TripleJ tends to have a good mix of quality and breadth in its music selection and is arguably the reason that the Australian music industry as a whole has punched above its weight for so long. Which is why I'll be looking at the TripleJ data in particular. Let's calculate which song and which artist had the most airtime throughout 2022. Usage install abc-radio-wrapper pypi package pip install abc-radio-wrapper import pandas as pd import abc_radio_wrapper ABC = abc_radio_wrapper . ABCRadio () startDate : datetime = datetime . fromisoformat ( \"2022-01-01T00:00:00+00:00\" ) endDate : datetime = datetime . fromisoformat ( \"2022-12-31T23:59:59+00:00\" ) radio_play_list = [] #search through 1 year period of triplej songs, set limit to 100 for faster results (default is 10) for search_result in ABC . continuous_search ( from_ = startDate , to = endDate , station = \"triplej\" , limit = 100 ): print ( search_result . offset / search_result . total ) for radio_play in search_result . radio_songs : artists = [ artist . name for artist in radio_play . song . artists ] if len ( artists ) == 0 : continue radio_play_list . append ({ \"song\" : radio_play . song . title , \"time\" : radio_play . played_time , \"artist\" : artists [ 0 ]}) df = pd . DataFrame ( radio_play_list ) #get the top 20 artists by playtime print ( df . groupby ( 'artist' ) . count () . sort_values ( 'time' , ascending = False )[ 0 : 20 ] ) song time artists artist Art Vs Science 876 876 876 Spacey Jane 649 649 649 The Wombats 562 562 562 Ball Park Music 519 519 519 Vance Joy 508 508 508 Flume 485 485 485 Ocean Alley 468 468 468 beabadoobee 453 453 453 Wet Leg 437 437 437 Maggie Rogers 431 431 431 G Flip 426 426 426 Charli XCX 421 421 421 Kendrick Lamar 412 412 412 Eliza & The Delusionals 409 409 409 Foals 407 407 407 Sycco 406 406 406 Beddy Rays 395 395 395 merci , mercy 394 394 394 Lil Nas X 389 389 389 Mura Masa 382 382 382 So what could you use this data for? From here if you were to pick a particular time interval you could imagine integrating with the youtube or spotify API to create a playlist for a certain day or month. I think a similar method is already used to compile the hottest 100 playlists on youtube after each January, at least I'd hope people weren't adding each item by hand. You could also challenge my assumption that TripleJ has a wide variety of music. One method would be to calculate the Gini Impurity of the song catalouge where each artist is it's own category. You'd need to compare the results with other radio stations or other periods of time. And a convenience function for matching a song to a youtube video. generate a youtube video from a song title import requests import urllib.parse def get_youtube_url ( song_name : str , apikey : str ) -> str : r = requests . get ( \"https://youtube.googleapis.com/youtube/v3/search?q=\" + urllib . parse . quote_plus ( song_name ) + \"&key=\" + apikey ) video_id = r . json ()[ 'items' ][ 0 ][ 'id' ][ 'videoId' ] return \"https://www.youtube.com/watch?v=\" + video_id","title":"ABC Radio"},{"location":"ABC%20Radio/#abc-radio","text":"I wrote a python library for searching and parsing through the historical catalgoue of songs played on ABC (Australian Broadcasting Corporation) radio channels. This is especially timely since TripleJ will be releasing it's \"Hottest 100\" playlist for 2022 in a few days. I'm particularly interested in the TripleJ data due to it aligning with my music preferences. TripleJ tends to have a good mix of quality and breadth in its music selection and is arguably the reason that the Australian music industry as a whole has punched above its weight for so long. Which is why I'll be looking at the TripleJ data in particular. Let's calculate which song and which artist had the most airtime throughout 2022.","title":"ABC Radio"},{"location":"ABC%20Radio/#usage","text":"install abc-radio-wrapper pypi package pip install abc-radio-wrapper import pandas as pd import abc_radio_wrapper ABC = abc_radio_wrapper . ABCRadio () startDate : datetime = datetime . fromisoformat ( \"2022-01-01T00:00:00+00:00\" ) endDate : datetime = datetime . fromisoformat ( \"2022-12-31T23:59:59+00:00\" ) radio_play_list = [] #search through 1 year period of triplej songs, set limit to 100 for faster results (default is 10) for search_result in ABC . continuous_search ( from_ = startDate , to = endDate , station = \"triplej\" , limit = 100 ): print ( search_result . offset / search_result . total ) for radio_play in search_result . radio_songs : artists = [ artist . name for artist in radio_play . song . artists ] if len ( artists ) == 0 : continue radio_play_list . append ({ \"song\" : radio_play . song . title , \"time\" : radio_play . played_time , \"artist\" : artists [ 0 ]}) df = pd . DataFrame ( radio_play_list ) #get the top 20 artists by playtime print ( df . groupby ( 'artist' ) . count () . sort_values ( 'time' , ascending = False )[ 0 : 20 ] ) song time artists artist Art Vs Science 876 876 876 Spacey Jane 649 649 649 The Wombats 562 562 562 Ball Park Music 519 519 519 Vance Joy 508 508 508 Flume 485 485 485 Ocean Alley 468 468 468 beabadoobee 453 453 453 Wet Leg 437 437 437 Maggie Rogers 431 431 431 G Flip 426 426 426 Charli XCX 421 421 421 Kendrick Lamar 412 412 412 Eliza & The Delusionals 409 409 409 Foals 407 407 407 Sycco 406 406 406 Beddy Rays 395 395 395 merci , mercy 394 394 394 Lil Nas X 389 389 389 Mura Masa 382 382 382 So what could you use this data for? From here if you were to pick a particular time interval you could imagine integrating with the youtube or spotify API to create a playlist for a certain day or month. I think a similar method is already used to compile the hottest 100 playlists on youtube after each January, at least I'd hope people weren't adding each item by hand. You could also challenge my assumption that TripleJ has a wide variety of music. One method would be to calculate the Gini Impurity of the song catalouge where each artist is it's own category. You'd need to compare the results with other radio stations or other periods of time. And a convenience function for matching a song to a youtube video. generate a youtube video from a song title import requests import urllib.parse def get_youtube_url ( song_name : str , apikey : str ) -> str : r = requests . get ( \"https://youtube.googleapis.com/youtube/v3/search?q=\" + urllib . parse . quote_plus ( song_name ) + \"&key=\" + apikey ) video_id = r . json ()[ 'items' ][ 0 ][ 'id' ][ 'videoId' ] return \"https://www.youtube.com/watch?v=\" + video_id","title":"Usage"},{"location":"Golden%20Rule%20Of%20Time/","text":"The Golden Rule of Time Why do some people in safe jobs work long nights with no chance of reward? Why do we espouse certain values that we don't follow? Recently, I read \"The Time Paradox\" by Zimbardo and Boyd. I enjoy reading pop psychology as long as they have a bit more depth and legitimacy than power posing, fortunately Boyd and Zimbardo's writing about the psychology of time is profound. Although it is not the focus of the book, the section I enjoyed the most was their discussion of \"The Golden Rule of Time\". The golden rule of time goes something like this: \"Spend your time the way you wish others would spend their time.\" A reference point might be more useful, imagine your son or daughter has to make the choice of working until late at a job with no prospect of promotion in the immediate future or they can go home on time? Almost everyone would wish that their child enjoyed their freetime yet that is not the choice that many people make in reality. The golden rule of time is a way of framing the strangeness of these choices and clarifying the decision making process. As to the questions in the first paragraph, I have no good answer. Maybe it is due to Freud's hypothesized fear of death, which exists on such a subconscious level that we don't consider it when speaking of others. Maybe we are at our most rational when we consider other people. With 'The Golden Rule of Time' in your toolbelt I hope that you can use that rationality, usually reserved for others, on yourself. The Time Paradox: The New Psychology of Time That Will Change Your Life","title":"The Golden Rule of Time"},{"location":"Golden%20Rule%20Of%20Time/#the-golden-rule-of-time","text":"Why do some people in safe jobs work long nights with no chance of reward? Why do we espouse certain values that we don't follow? Recently, I read \"The Time Paradox\" by Zimbardo and Boyd. I enjoy reading pop psychology as long as they have a bit more depth and legitimacy than power posing, fortunately Boyd and Zimbardo's writing about the psychology of time is profound. Although it is not the focus of the book, the section I enjoyed the most was their discussion of \"The Golden Rule of Time\". The golden rule of time goes something like this: \"Spend your time the way you wish others would spend their time.\" A reference point might be more useful, imagine your son or daughter has to make the choice of working until late at a job with no prospect of promotion in the immediate future or they can go home on time? Almost everyone would wish that their child enjoyed their freetime yet that is not the choice that many people make in reality. The golden rule of time is a way of framing the strangeness of these choices and clarifying the decision making process. As to the questions in the first paragraph, I have no good answer. Maybe it is due to Freud's hypothesized fear of death, which exists on such a subconscious level that we don't consider it when speaking of others. Maybe we are at our most rational when we consider other people. With 'The Golden Rule of Time' in your toolbelt I hope that you can use that rationality, usually reserved for others, on yourself. The Time Paradox: The New Psychology of Time That Will Change Your Life","title":"The Golden Rule of Time"},{"location":"GradIO/","text":"Gradio I've been taking Jeremy Howard's \"Practical Deep Learning for Coders\" . The course is practical so it's no surprise that it discusses deployment during Lesson Two and in the process introduces the gradio package. Gradio makes a good use case for demonstrating deep learning models. The skill sets for starting up a webserver, creating a deep learning model and writing javascript for the presentation layer are different and having to do them all at once can be frustrating. Gradio takes care of the webserver and frontend part of the sandwich so you can focus on the meatier functionality bits. It's a good use case but there is still the issue of deploying the gradio application to the internet on a machine that can handle deep learning workloads. Thankfully last year Hugging Face added \"Hugging Face Spaces\" to their suite of tools. Each space has a limit of 16GB RAM and 8 CPU cores and hosts a docker container exposed to the internet. Last year I published a Korean language sentiment analysis pipeline to Hugging Faces, so I'll be using that as the basis for the gradio deployment. You can find the language model here . And the repository for the gradio application here . Since it's hosted on a site which allows CORS I can call to it from this frontend without any issues. One last thing about the model is that it's trained on movie review data, if you give it the phrase \"\ub208\ubb3c \ub0ac\ub2e4\" (\"I shed a tear\") it will predict it as having a positive sentiment with a >95% confidence rating. function updateValue(event) { text = document.getElementById('sentence').value; console.log(text); fetch(\"https://matthewburke-koreansentiment.hf.space/run/predict\", { method: \"POST\", headers: { \"Content-Type\": \"application/json\" }, body: JSON.stringify({ data: [ text, ] })}) .then(r => r.json()) .then( r => { var str = JSON.stringify(r.data, null, 2); // results.innerHTML = str; } ) } submitbutton.addEventListener('click', updateValue);","title":"GradIO"},{"location":"The%20Friendship%20Paradox/","text":"The Friendship Paradox On average your friends will have more friends than you. On average the football team you support will be one of the mosty popular football teams in the world. And the train or bus you took to work today was close to full capacity. In graph theory there is a concept called \"The Friendship Paradox\" which stated plainly goes something like this \"your friends have more friends than you\". If we were to measure this, the recipe would look like: Procedure Sample a random node (or ask a random person) Count the number of edges (i.e. 'degree') for that node (how many friends can you count) Calculate the degree for each neighbouring node (number of friends your friends have) Take the average of the neighbouring nodes degrees Assert that the average degree of neighbouring nodes is higher than the degree of the randomly sampled node. But it's also true that you or any other random node in the friendship graph could have more friends than the average node. The paradox occurs due to the difference in sampling methods. Let's take the example of passengers catching a bus to see how this happens. Imagine there are only two busses in the world. One that takes 50 passengers and the other that takes 5 passengers. If we ask each busdriver how many passengers they have we get an array of [50,5] for which the average is 27.5, a reasonable amount. But if we were to ask each passenger how many passengers are on the bus we get an array of [50, ..x48, 50, 5, 5, 5, 5, 5] with an average of: \\[ mean(degree) = (50 * 50 + 5 * 5)/(50+5) ~= 45.9... \\] We have two 'averages' for the same metric with vastly different results depending on if we sample each node or each edge (or which type of node in the case of a bipartite graph). Using \\(k\\) to represent the degrees of a node we can express the two alternatives as: Per node average \\[ \\sum_{i=0}^n \\frac{k_i}{n} = <k> \\] Per degree average \\[ \\frac{<k^2>}{<k>} \\] What should you measure? Which metric is more useful depends on the circumstances. If you are a busdriver then you will be assigned randomly to take on a bus route in which case the best estimate for the number of passengers at any one time is \\(<k>\\) but if you are a passenger then you are more likely to catch the bus during peek times for the same reason as everyone else, in which case the per degree average is a better estimate of how busy the bus will be. The same metaphor can works teachers and students in classrooms. And when we don't have a bipartite graph we end up with the friendship paradox.","title":"The Friendship Paradox"},{"location":"The%20Friendship%20Paradox/#the-friendship-paradox","text":"On average your friends will have more friends than you. On average the football team you support will be one of the mosty popular football teams in the world. And the train or bus you took to work today was close to full capacity. In graph theory there is a concept called \"The Friendship Paradox\" which stated plainly goes something like this \"your friends have more friends than you\". If we were to measure this, the recipe would look like:","title":"The Friendship Paradox"},{"location":"The%20Friendship%20Paradox/#procedure","text":"Sample a random node (or ask a random person) Count the number of edges (i.e. 'degree') for that node (how many friends can you count) Calculate the degree for each neighbouring node (number of friends your friends have) Take the average of the neighbouring nodes degrees Assert that the average degree of neighbouring nodes is higher than the degree of the randomly sampled node. But it's also true that you or any other random node in the friendship graph could have more friends than the average node. The paradox occurs due to the difference in sampling methods. Let's take the example of passengers catching a bus to see how this happens. Imagine there are only two busses in the world. One that takes 50 passengers and the other that takes 5 passengers. If we ask each busdriver how many passengers they have we get an array of [50,5] for which the average is 27.5, a reasonable amount. But if we were to ask each passenger how many passengers are on the bus we get an array of [50, ..x48, 50, 5, 5, 5, 5, 5] with an average of: \\[ mean(degree) = (50 * 50 + 5 * 5)/(50+5) ~= 45.9... \\] We have two 'averages' for the same metric with vastly different results depending on if we sample each node or each edge (or which type of node in the case of a bipartite graph). Using \\(k\\) to represent the degrees of a node we can express the two alternatives as:","title":"Procedure"},{"location":"The%20Friendship%20Paradox/#per-node-average","text":"\\[ \\sum_{i=0}^n \\frac{k_i}{n} = <k> \\]","title":"Per node average"},{"location":"The%20Friendship%20Paradox/#per-degree-average","text":"\\[ \\frac{<k^2>}{<k>} \\]","title":"Per degree average"},{"location":"The%20Friendship%20Paradox/#what-should-you-measure","text":"Which metric is more useful depends on the circumstances. If you are a busdriver then you will be assigned randomly to take on a bus route in which case the best estimate for the number of passengers at any one time is \\(<k>\\) but if you are a passenger then you are more likely to catch the bus during peek times for the same reason as everyone else, in which case the per degree average is a better estimate of how busy the bus will be. The same metaphor can works teachers and students in classrooms. And when we don't have a bipartite graph we end up with the friendship paradox.","title":"What should you measure?"},{"location":"Unix%20Named%20Pipes/","text":"Unix Named Pipes Message Queues Message queues are an architectural pattern to decouple the process of gathering tasks and resolving tasks. There are a couple of reasons why you would use this design pattern. It might not be possible to complete the task within a single request reply loop or it might be easier to resolve tasks in a batch method due to large initial process startup costs. Assuming that the process that resolves the work is different to the one that gathers the work (i.e. is a message queue and not a task queue) then this is also a form of interprocess communication. Other options for interprocess communication could be JSON over HTTP (for processes across servers), shared memory such as Redis or the filesystem for processes that run on the same server. Unix Pipes In unix everything is a file, including pipes. As a brief review pipes are a way of turning stdin (read as \"standard input\") to stdout (\"standard output\"). The following script uses cat which takes a file or several files from stdin and prints to stdout. The pipe in the middle takes that stdout and converts it to stdin for the next process which is also cat and prints the text to stdout. 'cat' converts stdin to stdout and the pipe converts stdout to stdin, together they act like the identity function for any text. The following two commands do the same thing. cat book.txt cat book.txt | cat \ud83d\ude2e\u200d\ud83d\udca8 that was a lot to parse out. Pipes are powerful and efficient when each process can handle the data as a stream. We can compare the time taken to run a pipe of 20 cats versus the time taken to run a pipe of one cat. time cat book.txt #... 0.007 total time cat book.txt | cat | \ud83d\udc31x20 | cat #... 0.021 total More processes means more processing but there is a lot of time saved when you dont have to read from the file system each time. We could also create a strange pipe by using cat if we save the stdout to an intermediate file and then read that file as the stdin for the next process, this takes roughly twice the time and also twice the cpu power as the unix pipe version. cat book.txt > tempfile ; cat tempfile > tempfile ; cat tempfile > tempfile ; ... Writing to files is always an option, there is a huge overlap between pipes and files in terms of functionality. But one reason why files aren't appropriate is when we want to make sure that each line is processed exactly once even if several processes are accessing the content. Unix Named Pipes as a Message Queue Unix has a feature called 'Named Pipes' where we can give a pipe a name, let it persist in the file system. Unlike regular pipes named pipes can persist over time, and unlike regular files they have the guarantee that each line will only be processed once. named pipes in bash mkfifo newnamedpipe echo \"hi\" > newnamedpipe #open up new terminal cat newnamedpipe #output of \"hi\" cat newnamedpipe #no output, waiting for new message to join queue. Read and write to named pipes in Python import os PIPE_NAME = \"my_named_pipe\" def write_named_pipe ( pipe_name , message ): # Create the named pipe if it does not exist if not os . path . exists ( pipe_name ): os . mkfifo ( pipe_name ) # Open the named pipe with open ( pipe_name , \"w\" ) as pipe : # Write the message to the pipe pipe . write ( \"Hello, other process!\" ) pipe . flush () def read_named_pipe ( pipe_name ): # Open the named pipe for reading with open ( pipe_name , \"r\" ) as pipe : response = pipe . read () print ( response ) Wrap up You can use named pipes as a simple message queue on any unix distribution.","title":"Unix Named Pipes"},{"location":"Unix%20Named%20Pipes/#unix-named-pipes","text":"","title":"Unix Named Pipes"},{"location":"Unix%20Named%20Pipes/#message-queues","text":"Message queues are an architectural pattern to decouple the process of gathering tasks and resolving tasks. There are a couple of reasons why you would use this design pattern. It might not be possible to complete the task within a single request reply loop or it might be easier to resolve tasks in a batch method due to large initial process startup costs. Assuming that the process that resolves the work is different to the one that gathers the work (i.e. is a message queue and not a task queue) then this is also a form of interprocess communication. Other options for interprocess communication could be JSON over HTTP (for processes across servers), shared memory such as Redis or the filesystem for processes that run on the same server.","title":"Message Queues"},{"location":"Unix%20Named%20Pipes/#unix-pipes","text":"In unix everything is a file, including pipes. As a brief review pipes are a way of turning stdin (read as \"standard input\") to stdout (\"standard output\"). The following script uses cat which takes a file or several files from stdin and prints to stdout. The pipe in the middle takes that stdout and converts it to stdin for the next process which is also cat and prints the text to stdout. 'cat' converts stdin to stdout and the pipe converts stdout to stdin, together they act like the identity function for any text. The following two commands do the same thing. cat book.txt cat book.txt | cat \ud83d\ude2e\u200d\ud83d\udca8 that was a lot to parse out. Pipes are powerful and efficient when each process can handle the data as a stream. We can compare the time taken to run a pipe of 20 cats versus the time taken to run a pipe of one cat. time cat book.txt #... 0.007 total time cat book.txt | cat | \ud83d\udc31x20 | cat #... 0.021 total More processes means more processing but there is a lot of time saved when you dont have to read from the file system each time. We could also create a strange pipe by using cat if we save the stdout to an intermediate file and then read that file as the stdin for the next process, this takes roughly twice the time and also twice the cpu power as the unix pipe version. cat book.txt > tempfile ; cat tempfile > tempfile ; cat tempfile > tempfile ; ... Writing to files is always an option, there is a huge overlap between pipes and files in terms of functionality. But one reason why files aren't appropriate is when we want to make sure that each line is processed exactly once even if several processes are accessing the content.","title":"Unix Pipes"},{"location":"Unix%20Named%20Pipes/#unix-named-pipes-as-a-message-queue","text":"Unix has a feature called 'Named Pipes' where we can give a pipe a name, let it persist in the file system. Unlike regular pipes named pipes can persist over time, and unlike regular files they have the guarantee that each line will only be processed once. named pipes in bash mkfifo newnamedpipe echo \"hi\" > newnamedpipe #open up new terminal cat newnamedpipe #output of \"hi\" cat newnamedpipe #no output, waiting for new message to join queue. Read and write to named pipes in Python import os PIPE_NAME = \"my_named_pipe\" def write_named_pipe ( pipe_name , message ): # Create the named pipe if it does not exist if not os . path . exists ( pipe_name ): os . mkfifo ( pipe_name ) # Open the named pipe with open ( pipe_name , \"w\" ) as pipe : # Write the message to the pipe pipe . write ( \"Hello, other process!\" ) pipe . flush () def read_named_pipe ( pipe_name ): # Open the named pipe for reading with open ( pipe_name , \"r\" ) as pipe : response = pipe . read () print ( response )","title":"Unix Named Pipes as a Message Queue"},{"location":"Unix%20Named%20Pipes/#wrap-up","text":"You can use named pipes as a simple message queue on any unix distribution.","title":"Wrap up"},{"location":"copulas/","text":"Copulas Copulas are a way of estimating the multivariate distribution of independently modelled univariate distributions. They are a useful way of modelling the joint distribution of a set of random variables. Warning Gaussian copulas were used widely in risk modelling during the GFC, the joint probability estimates will only be as good as your marginal probability modelling and appropriateness of the copula. Going through an example might be useful. Imagine we have $1000 in AAPL stock and $1000 in MSFT stock. What is the chance that they both drop by 5% on the same day? We could approach this question using a multivariate bayesian approach, model a hidden factor that affects both the prices of MSFT and AAPL, estimate the most likely parameters, sample from the posterior and record the proportion of times that we see a greater than 5% drop in each of the share prices. That can be a lot of work especially if we have already independently modelled the underlying assets. Build marginal distributions for the process you want to model First let's work out the distributions of each asset independently. We'll be using the normal distribution as that is conceptually the easiest to work through, although the copula approach works for any underlying distributions. Assuming the returns follow a normal distribution (not fat tailed enough, I know) then we will have to estimate the mean and standard deviation from the sample. \\[ x \\sim \\mathcal{n}(\\mu,\\,\\sigma^{2})\\,. \\] Get the paramters for AAPL and MSFT returns distribution import yfinance as yf tickers = yf . Tickers ( 'msft aapl' ) df = tickers . history ( period = \"5y\" ) aapl_returns = df [ 'Close' ][ 'AAPL' ] . pct_change () . dropna () msft_returns = df [ 'Close' ][ 'MSFT' ] . pct_change () . dropna () print ( \"AAPL return distribution\" ) print ( aapl_returns . describe ()) # AAPL return distribution count 1258.000000 mean 0.001252 std 0.020985 min - 0.128647 25 % - 0.008779 50 % 0.001140 75 % 0.012366 max 0.119808 print ( \"MSFT return distribution\" ) print ( msft_returns . describe ()) count 1258.000000 mean 0.001149 std 0.019466 min - 0.147390 25 % - 0.007827 50 % 0.001289 75 % 0.010938 max 0.142169 make CDF for AAPL and MSFT returns from scipy.stats import norm def msft_return_probability ( returns : float ) -> float : \"\"\"CDF of MSFT returns >>> round(msft_return_probability(-0.05),2) 0.01 \"\"\" mean = 0.00115 std = 0.01947 zscore = ( returns - mean ) / std return norm . cdf ( zscore ) def aapl_return_probability ( returns : float ) -> float : \"\"\"CDF of AAPL returns >>> round(aapl_return_probability(-0.05),2) 0.03 \"\"\" mean = 0.001252 std = 0.020985 zscore = ( returns - mean ) / std return norm . cdf ( zscore ) Pick a copula to calculate the joint distribution We now have our underlying marginal distributions for each set of assets. Now we have to somehow combine them to form a joint distribution. The easiest way would be to assume that they are independent of each other. We could write an independent copula like this: def independent_copula ( u , v ): return u * v Using this copula the chance of them both going down at least 5% would be 0.01 * 0.03 or 3/10000. If they were perfectly correlated then the chance of them both going down at least 5% would be a much higher number, something like: def correlated_copula ( u , v ): return max ( u , v ) Note When we read this function for the inputs correlated_copula(0.8,0.3) we should say \"What is the chance of at least the 80th percentile of returns and at least the 30th percentile of returns?\" If they are completely correlated then when one distribution returns the 80th percentile then the other distribution must also return at least the 80th percentile. correlated_copula ( 0.8 , 0.3 ) == correlated_copula ( 0.8 , 0.8 ) Which gives us a much more realistic 3% chance, this is problematic in that we cannot prove that they are completely correlated and likely overestimates the risk. Going by this metric we could reduce our risk by dropping AAPL entirely and putting all our money in MSFT, this is the opposite of the mantra that \"Diversification is the only free lunch.\u201d Let's make the assumption that they are inversely correlated, which gives us the following copula: def inversely_correlated_copula ( u , v ): return max ( u + v - 1 , 0 ) Accordingly if AAPL were truly uncorrelated with MSFT then there would be no days in which they both lose money and the joint probability of a 5% loss on both assets would be 0. An alternative copula that we can use is the Gumbel copula which takes into account the correlation of the underlying marginal distributions. You can see the code below. import numpy as np def gumbel_copula ( u , v , alpha ): u = np . asarray ( u ) v = np . asarray ( v ) # compute copula copula = np . exp ( - ((( - np . log ( u )) ** alpha + ( - np . log ( v )) ** alpha )) ** ( 1 / alpha )) return copula For the Gumbel copula, when alpha is equal to one then the copula behaves the same as the independent copula. When alpha approaches infinity it behaves like the inversely correlated copula. So you can guess that alpha is related to the correlation of the series u and v. To calculate alpha we first calculate the Kendall tau value of the two series. Kendall's tau can be described as a type of rank correlation coefficient. When making the calculation we use the probability values and not the values of the underlying returns i.e. from scipy.stats import kendalltau def calculate_alpha ( u , v ): u = np . array ( u ) v = np . array ( v ) tau = kendalltau ( u , v ) . correlation return 1 / ( 1 - tau ) Apply the copula to find the joint probability of two events Let's create the probability series from the returns series and calculate our alpha value for MSFT and AAPL: calculate the joint probabilites for different copula alpha = calculate_alpha ( aapl_returns . apply ( lambda x : aapl_return_probability ( x )), msft_returns . apply ( lambda x : msft_return_probability ( x ))) print ( alpha ) #2.2015... print ( aapl_return_probability ( - 0.05 ) * msft_return_probability ( - 0.05 )) # 0.0002.. print ( gumbel_copula ( aapl_return_probability ( - 0.05 ), msft_return_probability ( - 0.05 )) # 0.0032 Even with our poor modeling of the marginal distributions the Gumbel copula is able to give us a much better estimation of the probability that both companies would drop 5% in a single day. Looking at the history over the last 5 years there have been 7 out of 1258 trading days in which both companies dropped 5% (0.0056 by proportion). Graphing the result independent copula graph gumbel copula graph X , Y = np . mgrid [ 0 : 0.5 : 0.05 , 0 : 0.5 : 0.05 ] import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X , Y , independent_copula ( X , Y ), c = 'red' ) plt . savefig ( \"independent_copula.png\" ) X , Y = np . mgrid [ 0 : 0.5 : 0.05 , 0 : 0.5 : 0.05 ] import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X , Y , gumbel_copula ( X , Y , alpha ), c = 'red' ) plt . savefig ( \"gumbel_copula.png\" ) The graph is steeper for the Gumbel copula, which means the joint probability is higher for two unlikely but correlated events. Wrap up Copula's are used in risk management for their flexibility in combining a wide range of probability models into a joint probability. Even with a simple marginal probability model the combined copula was reasonably accurate in estimating the joint distribution for correlated assets. You could use copulas for predicting the network load on two related software services or for calculating the risk of correlated assets that are modelled differently. Below I have included a chart comparing the probability estimates for the Gumbel and Independent copulas for when u = v . Once again the Gumbel copula estimates a higher probability of two correlated but low probability events. // Initialize the echarts instance based on the prepared dom var myChart = echarts.init(document.getElementById('chart')); // Specify the configuration items and data for the chart option = { title:{ text: 'Gumbel and Independent copula', subtext: 'y=Copula(x,x)' }, tooltip: { trigger: 'item', formatter: '{a} {b},{c} ' }, xAxis: { data: [0. , 0.025, 0.05 , 0.075, 0.1 , 0.125, 0.15 , 0.175, 0.2 , 0.225, 0.25 , 0.275, 0.3 , 0.325, 0.35 , 0.375, 0.4 , 0.425, 0.45 , 0.475], name:'' }, yAxis: {name: ''}, series: [ { data: [0. , 0.000625, 0.0025 , 0.005625, 0.01 , 0.015625, 0.0225 , 0.030625, 0.04 , 0.050625, 0.0625 , 0.075625, 0.09 , 0.105625, 0.1225 , 0.140625, 0.16 , 0.180625, 0.2025 , 0.225625], type: 'line', stack: 'x', areaStyle: {}, name:'Independent' }, { data: [0. , 0.0065148 , 0.01677541, 0.02917153, 0.04319615, 0.0585712 , 0.07511579, 0.09270087, 0.11122874, 0.13062234, 0.150819 , 0.17176657, 0.19342082, 0.21574365, 0.23870185, 0.26226616, 0.28641054, 0.31111165, 0.33634844, 0.36210178], type: 'line', stack: 'x', areaStyle: {}, name:'Gumbel' } ] }; myChart.setOption(option);","title":"Copulas"},{"location":"copulas/#copulas","text":"Copulas are a way of estimating the multivariate distribution of independently modelled univariate distributions. They are a useful way of modelling the joint distribution of a set of random variables. Warning Gaussian copulas were used widely in risk modelling during the GFC, the joint probability estimates will only be as good as your marginal probability modelling and appropriateness of the copula. Going through an example might be useful. Imagine we have $1000 in AAPL stock and $1000 in MSFT stock. What is the chance that they both drop by 5% on the same day? We could approach this question using a multivariate bayesian approach, model a hidden factor that affects both the prices of MSFT and AAPL, estimate the most likely parameters, sample from the posterior and record the proportion of times that we see a greater than 5% drop in each of the share prices. That can be a lot of work especially if we have already independently modelled the underlying assets.","title":"Copulas"},{"location":"copulas/#build-marginal-distributions-for-the-process-you-want-to-model","text":"First let's work out the distributions of each asset independently. We'll be using the normal distribution as that is conceptually the easiest to work through, although the copula approach works for any underlying distributions. Assuming the returns follow a normal distribution (not fat tailed enough, I know) then we will have to estimate the mean and standard deviation from the sample. \\[ x \\sim \\mathcal{n}(\\mu,\\,\\sigma^{2})\\,. \\] Get the paramters for AAPL and MSFT returns distribution import yfinance as yf tickers = yf . Tickers ( 'msft aapl' ) df = tickers . history ( period = \"5y\" ) aapl_returns = df [ 'Close' ][ 'AAPL' ] . pct_change () . dropna () msft_returns = df [ 'Close' ][ 'MSFT' ] . pct_change () . dropna () print ( \"AAPL return distribution\" ) print ( aapl_returns . describe ()) # AAPL return distribution count 1258.000000 mean 0.001252 std 0.020985 min - 0.128647 25 % - 0.008779 50 % 0.001140 75 % 0.012366 max 0.119808 print ( \"MSFT return distribution\" ) print ( msft_returns . describe ()) count 1258.000000 mean 0.001149 std 0.019466 min - 0.147390 25 % - 0.007827 50 % 0.001289 75 % 0.010938 max 0.142169 make CDF for AAPL and MSFT returns from scipy.stats import norm def msft_return_probability ( returns : float ) -> float : \"\"\"CDF of MSFT returns >>> round(msft_return_probability(-0.05),2) 0.01 \"\"\" mean = 0.00115 std = 0.01947 zscore = ( returns - mean ) / std return norm . cdf ( zscore ) def aapl_return_probability ( returns : float ) -> float : \"\"\"CDF of AAPL returns >>> round(aapl_return_probability(-0.05),2) 0.03 \"\"\" mean = 0.001252 std = 0.020985 zscore = ( returns - mean ) / std return norm . cdf ( zscore )","title":"Build marginal distributions for the process you want to model"},{"location":"copulas/#pick-a-copula-to-calculate-the-joint-distribution","text":"We now have our underlying marginal distributions for each set of assets. Now we have to somehow combine them to form a joint distribution. The easiest way would be to assume that they are independent of each other. We could write an independent copula like this: def independent_copula ( u , v ): return u * v Using this copula the chance of them both going down at least 5% would be 0.01 * 0.03 or 3/10000. If they were perfectly correlated then the chance of them both going down at least 5% would be a much higher number, something like: def correlated_copula ( u , v ): return max ( u , v ) Note When we read this function for the inputs correlated_copula(0.8,0.3) we should say \"What is the chance of at least the 80th percentile of returns and at least the 30th percentile of returns?\" If they are completely correlated then when one distribution returns the 80th percentile then the other distribution must also return at least the 80th percentile. correlated_copula ( 0.8 , 0.3 ) == correlated_copula ( 0.8 , 0.8 ) Which gives us a much more realistic 3% chance, this is problematic in that we cannot prove that they are completely correlated and likely overestimates the risk. Going by this metric we could reduce our risk by dropping AAPL entirely and putting all our money in MSFT, this is the opposite of the mantra that \"Diversification is the only free lunch.\u201d Let's make the assumption that they are inversely correlated, which gives us the following copula: def inversely_correlated_copula ( u , v ): return max ( u + v - 1 , 0 ) Accordingly if AAPL were truly uncorrelated with MSFT then there would be no days in which they both lose money and the joint probability of a 5% loss on both assets would be 0. An alternative copula that we can use is the Gumbel copula which takes into account the correlation of the underlying marginal distributions. You can see the code below. import numpy as np def gumbel_copula ( u , v , alpha ): u = np . asarray ( u ) v = np . asarray ( v ) # compute copula copula = np . exp ( - ((( - np . log ( u )) ** alpha + ( - np . log ( v )) ** alpha )) ** ( 1 / alpha )) return copula For the Gumbel copula, when alpha is equal to one then the copula behaves the same as the independent copula. When alpha approaches infinity it behaves like the inversely correlated copula. So you can guess that alpha is related to the correlation of the series u and v. To calculate alpha we first calculate the Kendall tau value of the two series. Kendall's tau can be described as a type of rank correlation coefficient. When making the calculation we use the probability values and not the values of the underlying returns i.e. from scipy.stats import kendalltau def calculate_alpha ( u , v ): u = np . array ( u ) v = np . array ( v ) tau = kendalltau ( u , v ) . correlation return 1 / ( 1 - tau )","title":"Pick a copula to calculate the joint distribution"},{"location":"copulas/#apply-the-copula-to-find-the-joint-probability-of-two-events","text":"Let's create the probability series from the returns series and calculate our alpha value for MSFT and AAPL: calculate the joint probabilites for different copula alpha = calculate_alpha ( aapl_returns . apply ( lambda x : aapl_return_probability ( x )), msft_returns . apply ( lambda x : msft_return_probability ( x ))) print ( alpha ) #2.2015... print ( aapl_return_probability ( - 0.05 ) * msft_return_probability ( - 0.05 )) # 0.0002.. print ( gumbel_copula ( aapl_return_probability ( - 0.05 ), msft_return_probability ( - 0.05 )) # 0.0032 Even with our poor modeling of the marginal distributions the Gumbel copula is able to give us a much better estimation of the probability that both companies would drop 5% in a single day. Looking at the history over the last 5 years there have been 7 out of 1258 trading days in which both companies dropped 5% (0.0056 by proportion).","title":"Apply the copula to find the joint probability of two events"},{"location":"copulas/#graphing-the-result","text":"independent copula graph gumbel copula graph X , Y = np . mgrid [ 0 : 0.5 : 0.05 , 0 : 0.5 : 0.05 ] import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X , Y , independent_copula ( X , Y ), c = 'red' ) plt . savefig ( \"independent_copula.png\" ) X , Y = np . mgrid [ 0 : 0.5 : 0.05 , 0 : 0.5 : 0.05 ] import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X , Y , gumbel_copula ( X , Y , alpha ), c = 'red' ) plt . savefig ( \"gumbel_copula.png\" ) The graph is steeper for the Gumbel copula, which means the joint probability is higher for two unlikely but correlated events.","title":"Graphing the result"},{"location":"copulas/#wrap-up","text":"Copula's are used in risk management for their flexibility in combining a wide range of probability models into a joint probability. Even with a simple marginal probability model the combined copula was reasonably accurate in estimating the joint distribution for correlated assets. You could use copulas for predicting the network load on two related software services or for calculating the risk of correlated assets that are modelled differently. Below I have included a chart comparing the probability estimates for the Gumbel and Independent copulas for when u = v . Once again the Gumbel copula estimates a higher probability of two correlated but low probability events. // Initialize the echarts instance based on the prepared dom var myChart = echarts.init(document.getElementById('chart')); // Specify the configuration items and data for the chart option = { title:{ text: 'Gumbel and Independent copula', subtext: 'y=Copula(x,x)' }, tooltip: { trigger: 'item', formatter: '{a} {b},{c} ' }, xAxis: { data: [0. , 0.025, 0.05 , 0.075, 0.1 , 0.125, 0.15 , 0.175, 0.2 , 0.225, 0.25 , 0.275, 0.3 , 0.325, 0.35 , 0.375, 0.4 , 0.425, 0.45 , 0.475], name:'' }, yAxis: {name: ''}, series: [ { data: [0. , 0.000625, 0.0025 , 0.005625, 0.01 , 0.015625, 0.0225 , 0.030625, 0.04 , 0.050625, 0.0625 , 0.075625, 0.09 , 0.105625, 0.1225 , 0.140625, 0.16 , 0.180625, 0.2025 , 0.225625], type: 'line', stack: 'x', areaStyle: {}, name:'Independent' }, { data: [0. , 0.0065148 , 0.01677541, 0.02917153, 0.04319615, 0.0585712 , 0.07511579, 0.09270087, 0.11122874, 0.13062234, 0.150819 , 0.17176657, 0.19342082, 0.21574365, 0.23870185, 0.26226616, 0.28641054, 0.31111165, 0.33634844, 0.36210178], type: 'line', stack: 'x', areaStyle: {}, name:'Gumbel' } ] }; myChart.setOption(option);","title":"Wrap up"},{"location":"geometric%20distribution/","text":"Geometric Distribution interview question If a job candidate has a 1% chance of passing an interview, what is the chance that he receives a job offer after 100 interviews? The first iteration I saw of this question required solving the problem without a calculator. The exact solution is easy with a bit of probability theory. The approximation requires a bit of intuition, and a much longer explanation. The probability that they succeed on the Nth interview is the probability of succeeding during an inteview after failing (n-1) interviews i.e.the pmf is \\[ f(n) = p*(1-p)^{n-1} \\] And the chance that they would have succeeded on at least 1 of the N interviews is the inverse of them failing every interview i.e. the cdf is \\[ f(N \\le n) = 1 - (1-p)^n \\] If we plug in our values for p=0.01 and n = 100 With a calculator the answer to two decimal places is 0.63. Given that p =1/n in the question my first thought was to find a general solution for all values where p is the inverse of n, i.e. p * n =1. Intuitively whether n is 100 or 1000 the answer should be approximately the same since the expected amount of interview successes is the same (p * n). The mathematical definition of e is close to what we want but we have the wrong sign in the brackets. \\[ e = \\lim_{n \\to \\infty} (1+1/n)^n \\] Let's introduce an approximation: \\[ \\lim_{x \\to \\infty} (1 + 1/x)^n = (1 + n/x) \\] Note in this approximation we can replace x with -x and n with -n and the approximation still holds i.e. for numbers close to 1 the power operation is similar to multiplication. correect to the 5th decimal place assert abs ( 1.001 ** 4 ) - 1.004 < 0.00001 let's use this to get the negative version of the definition of e. Taking n to the limit of infinity: \\[ (1-1/n)^n = (1+1/n)^{-n} = 1/e \\] Each time we swap a sign we are taking the reciprocal. If we take the reciprocal twice we get the original (applying any inverse function twice is the identity function). \\[ e = \\lim_{n \\to \\infty} (1-1/n)^{-n} \\] \\[ 1/e = \\lim_{n \\to \\infty} (1-1/n)^{n} \\] with this second limit we are getting very close to the geometric distributions CDF, let's swap out the expression. for p * n =1 \\[ CDF = 1 - (1-p)^n = 1 - 1/e \\] e approximation vs calculator from math import e calculated = 1 - ( 1 - 0.01 ) ** 100 #apprxomiately 0.63 estimated = 1 - 1 / e #approximately 0.63 assert abs ( calculated - estimated ) < 0.01 This approximation holds roughly to the number of decimal places of p. The expression is particularly nice when we assume n*p =1, but even without the assumption, we could still receive an answer in terms of e: \\[ CDF = 1 - 1/(e^{p*n}) \\]","title":"Geometric Distribution"},{"location":"geometric%20distribution/#geometric-distribution","text":"interview question If a job candidate has a 1% chance of passing an interview, what is the chance that he receives a job offer after 100 interviews? The first iteration I saw of this question required solving the problem without a calculator. The exact solution is easy with a bit of probability theory. The approximation requires a bit of intuition, and a much longer explanation. The probability that they succeed on the Nth interview is the probability of succeeding during an inteview after failing (n-1) interviews i.e.the pmf is \\[ f(n) = p*(1-p)^{n-1} \\] And the chance that they would have succeeded on at least 1 of the N interviews is the inverse of them failing every interview i.e. the cdf is \\[ f(N \\le n) = 1 - (1-p)^n \\] If we plug in our values for p=0.01 and n = 100 With a calculator the answer to two decimal places is 0.63. Given that p =1/n in the question my first thought was to find a general solution for all values where p is the inverse of n, i.e. p * n =1. Intuitively whether n is 100 or 1000 the answer should be approximately the same since the expected amount of interview successes is the same (p * n). The mathematical definition of e is close to what we want but we have the wrong sign in the brackets. \\[ e = \\lim_{n \\to \\infty} (1+1/n)^n \\] Let's introduce an approximation: \\[ \\lim_{x \\to \\infty} (1 + 1/x)^n = (1 + n/x) \\] Note in this approximation we can replace x with -x and n with -n and the approximation still holds i.e. for numbers close to 1 the power operation is similar to multiplication. correect to the 5th decimal place assert abs ( 1.001 ** 4 ) - 1.004 < 0.00001 let's use this to get the negative version of the definition of e. Taking n to the limit of infinity: \\[ (1-1/n)^n = (1+1/n)^{-n} = 1/e \\] Each time we swap a sign we are taking the reciprocal. If we take the reciprocal twice we get the original (applying any inverse function twice is the identity function). \\[ e = \\lim_{n \\to \\infty} (1-1/n)^{-n} \\] \\[ 1/e = \\lim_{n \\to \\infty} (1-1/n)^{n} \\] with this second limit we are getting very close to the geometric distributions CDF, let's swap out the expression. for p * n =1 \\[ CDF = 1 - (1-p)^n = 1 - 1/e \\] e approximation vs calculator from math import e calculated = 1 - ( 1 - 0.01 ) ** 100 #apprxomiately 0.63 estimated = 1 - 1 / e #approximately 0.63 assert abs ( calculated - estimated ) < 0.01 This approximation holds roughly to the number of decimal places of p. The expression is particularly nice when we assume n*p =1, but even without the assumption, we could still receive an answer in terms of e: \\[ CDF = 1 - 1/(e^{p*n}) \\]","title":"Geometric Distribution"},{"location":"meritocracy/","text":"Meritocracy I read Michael Sandel's book \"The Tyranny of Merit\" based on the recommendation of The Guardian's best books of 2020. The book is good and although it's focused on the American implementation of a meritocratic system it's enough to reveal the inconsistent values that I see in Australia and to an even worse extent in South Korea. I am a benefactor of the meritocratic systems of the countries that I have lived in. I've received an expensive education. I work in an economy where my talents and intellectual interests are highly valued, which gives me the double benefit of status and wealth. But I could have been born in a situation where I wasn't able to receive an expensive education or live in an economy where my labour is valued. And neither case would be the result of my choices. While reading I felt the most pity for the current generation of Koreans that are in or about to enter their 20s. South Korea, as Sandel noted, might be the most meritocratic country in the world. The university admission exams are conducted yearly, those that can't reach their favoured universities will repeat the exam 2 or 3 times. As such, exam first-timers are often competing against people that have dedicated 3 years to the exam which makes their prospects even worse. Even amongst those that graduate from the top universities, a large number will choose to sit the biannual public administration exam. Some of these exams have an admission rate less than Harvard which is why some people typically devote years to the task despite no guarantee of success. On the other end of the spectrum there are the 3D jobs (Dirty, Dangerous and Difficult). The unofficial fourth 'D' is 'demeaning'. 3D jobs aren't inherently low paid or 'demeaning'. But in a conformist society where the be-all and end-all is academic rigour then those that refuse to play are considered losers. Sandel spends a lot of time comparing the education system to a merit sorting machine. Sorting and ranking is ultimately a mathematical discipline. Take the feature vector [x1,x2...xn] and the weighting vector [w1,w2,...wn], calculate the inner product and now you have your single number for ranking. In this analogy the features would be a student's abilities and the weight vector would be how each ability is valued in society. If there are a variety of weighting vectors then each student has options of optimising their own feature vector. But if the weight vector is always [1,0,...0] then students must compete on the x1 feature. Even if there are other weight vectors around they have to be made known when students start optimising for their futures. What ends up being measured is a students' ability to optimise i.e. study for a test. There is a correlation between test marks and good characteristics such as intelligence and hard work. But there is an even higher correlation for parents income, which makes test scores as a measure of ability entirely unfair.","title":"Meritocracy"},{"location":"meritocracy/#meritocracy","text":"I read Michael Sandel's book \"The Tyranny of Merit\" based on the recommendation of The Guardian's best books of 2020. The book is good and although it's focused on the American implementation of a meritocratic system it's enough to reveal the inconsistent values that I see in Australia and to an even worse extent in South Korea. I am a benefactor of the meritocratic systems of the countries that I have lived in. I've received an expensive education. I work in an economy where my talents and intellectual interests are highly valued, which gives me the double benefit of status and wealth. But I could have been born in a situation where I wasn't able to receive an expensive education or live in an economy where my labour is valued. And neither case would be the result of my choices. While reading I felt the most pity for the current generation of Koreans that are in or about to enter their 20s. South Korea, as Sandel noted, might be the most meritocratic country in the world. The university admission exams are conducted yearly, those that can't reach their favoured universities will repeat the exam 2 or 3 times. As such, exam first-timers are often competing against people that have dedicated 3 years to the exam which makes their prospects even worse. Even amongst those that graduate from the top universities, a large number will choose to sit the biannual public administration exam. Some of these exams have an admission rate less than Harvard which is why some people typically devote years to the task despite no guarantee of success. On the other end of the spectrum there are the 3D jobs (Dirty, Dangerous and Difficult). The unofficial fourth 'D' is 'demeaning'. 3D jobs aren't inherently low paid or 'demeaning'. But in a conformist society where the be-all and end-all is academic rigour then those that refuse to play are considered losers. Sandel spends a lot of time comparing the education system to a merit sorting machine. Sorting and ranking is ultimately a mathematical discipline. Take the feature vector [x1,x2...xn] and the weighting vector [w1,w2,...wn], calculate the inner product and now you have your single number for ranking. In this analogy the features would be a student's abilities and the weight vector would be how each ability is valued in society. If there are a variety of weighting vectors then each student has options of optimising their own feature vector. But if the weight vector is always [1,0,...0] then students must compete on the x1 feature. Even if there are other weight vectors around they have to be made known when students start optimising for their futures. What ends up being measured is a students' ability to optimise i.e. study for a test. There is a correlation between test marks and good characteristics such as intelligence and hard work. But there is an even higher correlation for parents income, which makes test scores as a measure of ability entirely unfair.","title":"Meritocracy"},{"location":"python_profiling/","text":"Python Profiling I've seen a few posts on Hacker News recently about writing fast and efficient code. Fortunately this example was written in Python which is probably the language that could save the most instruction cycles globally with a bit of optimization. I say this because Python is ubiquitous when it comes to writing prototype scripts that eventually get pushed to production under project deadlines. The rule of thumb that I like to use, and remind my coworkers, is that the order of code optimization is: Make it work - optimize for business functionality (MVP) Make it right - optimize for human readability (documentation, test cases, extendable) Make it fast - optimize for execution speed In the work of data science, the opportunities to get to the third stage are rare enough that they aren't covered in many learning materials but important enough that you need to know them. In this post i'll cover the profiling basics when using Python. Be aware that many profiling results are dependent on the current state of the computer running the profiler, it's best to run each result several times to normalize across background processes that may be happening. %timeit For those using Jupyter notebooks or an iPython environment, %timeit is the easiest way to start measuring execution time of functions and therefore being able to compare speeds of different solutions. timeit % timeit [ i ** 3 for i in range ( 100 )] #100000 loops, best of 5: 12.1 \u00b5s per loop %memit memit is the easiest way to start memory profiling in a Jupyter Notebook or iPython environment. memit ! pip install memory_profiler % load_ext memory_profiler % memit ( i ** 3 for i in range ( 10000 )) % memit [ i ** 3 for i in range ( 10000 )] /usr/bin/time /usr/bin/time is a unix utility to time the execution of any program. It doesn't have as much detail as the python specific profilers but can be used for all types of programs that run as a unix CLI. /usr/bin/time curl https://matthewburke.xyz >> /dev/null cProfile cProfile is part of the standard library and will show you the time spent in each function of a certain program. python -m cProfile -s cumulative script.py python -m cProfile -s cumulative script.py | grep script.py line_profiler Once you've worked out which function is using up the most CPU resources you can dive in deeper and investigate which line is using the most CPU resources with line_profiler. To run this you'll need to download the pip package line_profiler, add the @profile decorator to the functions you want to profile and finally run the line_profiler from the command line function_to_profile.py @profile def slow_add ( a : int , b : int ) -> int : time . sleep ( 1 ) return a + b kernprof -l function_to_profile.py See the github repo github repo for more information. memory_profiler The memory_profiler works in much the same way as the line_profiler. Have a look at the official documentation to get started. py-spy py-spy (note the hyphen) is an incredible piece of work which allows you to profile a python process as it is running without slowing down the process (too much). You can imagine how useful it would be when trying to determine what the bottleneck is for a web server when serving real traffic. You can find more information on the github repo .","title":"Python Profiling"},{"location":"python_profiling/#python-profiling","text":"I've seen a few posts on Hacker News recently about writing fast and efficient code. Fortunately this example was written in Python which is probably the language that could save the most instruction cycles globally with a bit of optimization. I say this because Python is ubiquitous when it comes to writing prototype scripts that eventually get pushed to production under project deadlines. The rule of thumb that I like to use, and remind my coworkers, is that the order of code optimization is: Make it work - optimize for business functionality (MVP) Make it right - optimize for human readability (documentation, test cases, extendable) Make it fast - optimize for execution speed In the work of data science, the opportunities to get to the third stage are rare enough that they aren't covered in many learning materials but important enough that you need to know them. In this post i'll cover the profiling basics when using Python. Be aware that many profiling results are dependent on the current state of the computer running the profiler, it's best to run each result several times to normalize across background processes that may be happening.","title":"Python Profiling"},{"location":"python_profiling/#timeit","text":"For those using Jupyter notebooks or an iPython environment, %timeit is the easiest way to start measuring execution time of functions and therefore being able to compare speeds of different solutions. timeit % timeit [ i ** 3 for i in range ( 100 )] #100000 loops, best of 5: 12.1 \u00b5s per loop","title":"%timeit"},{"location":"python_profiling/#memit","text":"memit is the easiest way to start memory profiling in a Jupyter Notebook or iPython environment. memit ! pip install memory_profiler % load_ext memory_profiler % memit ( i ** 3 for i in range ( 10000 )) % memit [ i ** 3 for i in range ( 10000 )]","title":"%memit"},{"location":"python_profiling/#usrbintime","text":"/usr/bin/time is a unix utility to time the execution of any program. It doesn't have as much detail as the python specific profilers but can be used for all types of programs that run as a unix CLI. /usr/bin/time curl https://matthewburke.xyz >> /dev/null","title":"/usr/bin/time"},{"location":"python_profiling/#cprofile","text":"cProfile is part of the standard library and will show you the time spent in each function of a certain program. python -m cProfile -s cumulative script.py python -m cProfile -s cumulative script.py | grep script.py","title":"cProfile"},{"location":"python_profiling/#line_profiler","text":"Once you've worked out which function is using up the most CPU resources you can dive in deeper and investigate which line is using the most CPU resources with line_profiler. To run this you'll need to download the pip package line_profiler, add the @profile decorator to the functions you want to profile and finally run the line_profiler from the command line function_to_profile.py @profile def slow_add ( a : int , b : int ) -> int : time . sleep ( 1 ) return a + b kernprof -l function_to_profile.py See the github repo github repo for more information.","title":"line_profiler"},{"location":"python_profiling/#memory_profiler","text":"The memory_profiler works in much the same way as the line_profiler. Have a look at the official documentation to get started.","title":"memory_profiler"},{"location":"python_profiling/#py-spy","text":"py-spy (note the hyphen) is an incredible piece of work which allows you to profile a python process as it is running without slowing down the process (too much). You can imagine how useful it would be when trying to determine what the bottleneck is for a web server when serving real traffic. You can find more information on the github repo .","title":"py-spy"}]}