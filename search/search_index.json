{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Matt's thoughts on things Who Hi, I'm Matt. I'm currently working in Korea for a fintech startup. I enjoy reading and writing about applied maths, computer science and korean culture. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Matt's thoughts on things"},{"location":"#matts-thoughts-on-things","text":"","title":"Matt's thoughts on things"},{"location":"#who","text":"Hi, I'm Matt. I'm currently working in Korea for a fintech startup. I enjoy reading and writing about applied maths, computer science and korean culture.","title":"Who"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"geometry/","text":"interview question If a job candidate has a 1% chance of passing an interview, what is the chance that he receives a job offer after 100 interviews? The first iteration I saw of this question required solving the problem without a calculator. The exact solution is easy with a bit of probability theory. The approximation requires a bit of intuition, and a much longer explanation. The probability that they succeed on the Nth interview is the probability of succeeding during an inteview after failing (n-1) interviews i.e. \\[ PMF = p*(1-p)^{n-1} \\] And the chance that they would have succeeded on at least 1 of the N interviews is the inverse of them failing every interview i.e. \\[ CDF = 1 - (1-p)^n \\] If we plug in our values for p=0.01 and n = 100 With a calculator the answer to two decimal places is 0.63. Given that p =1/n in the question my first thought was to find a general solution for all values where p is the inverse of n, i.e. p * n =1. Intuitively whether n is 100 or 1000 the answer should be approximately the same since the expected amount of interview successes is the same (p * n). The mathematical definition of e is close to what we want but we have the wrong sign in the brackets. \\[ e = \\lim_{n \\to \\infty} (1+1/n)^n \\] Let's introduce an approximation: \\[ \\lim_{x \\to \\infty} (1 + 1/x)^n = (1 + n/x) \\] Note in this approximation we can replace x with -x and n with -n and the approximation still holds i.e. for numbers very close to 1 the power operation is similar to multiplication. correect to the 5th decimal place assert abs ( 1.001 ** 4 ) - 1.004 < 0.00001 let's use this to get the negative version of the definition of e. Taking n to the limit of infinity: \\[ (1-1/n)^n = (1+1/n)^{-n} = 1/e \\] Each time we swap a sign we are taking the reciprocal. If we take the reciprocal twice we get the original (applying any inverse function twice is the identity function). \\[ e = \\lim_{n \\to \\infty} (1-1/n)^{-n} \\] \\[ 1/e = \\lim_{n \\to \\infty} (1-1/n)^{n} \\] with this second limit we are getting very close to the geometric distributions CDF, let's swap out the expression. for p * n =1 \\[ CDF = 1 - (1-p)^n = 1 - 1/e \\] e approximation vs calculator from math import e calculated = 1 - ( 1 - 0.01 ) ** 100 #apprxomiately 0.63 estimated = 1 - 1 / e #approximately 0.63 assert abs ( calculated - estimated ) < 0.01 This approximation holds roughly to the number of decimal places of p. The expression is particularly nice when we assume n*p =1, but even without the assumption, we could still receive an answer in terms of e: \\[ estimation = 1 - 1/(e^{p*n}) \\]","title":"Geometry"},{"location":"meritocracy/","text":"Meritocracy I recently read Michael Sandel's book \"The Tyranny of Merit\" on the recommendation of The Guardian's best books of 2020. The book is good and although it is mostly focused on the American implementation of a meritocratic system it was enough to reveal the inconsistent values that I see in Australia and to an even worse extent Korea. Truthfully, I am a benefactor of the meritocratic systems of the countries that I have lived in. I've received an expensive education. I work in an economy where my talents and intellectual interests are highly valued, which gives me the double benefit of status and wealth. But I could have just as easily been born in a situation where I wasn't able to receive an expensive education or live in an economy where my labour is valued. And neither case would be the result of my choices. While reading I felt the most pity for the current generation of Koreans that are in or about to enter their 20s. South Korea, as Sandel noted, might be the most meritocratic country in the world. The university admission exams are conducted yearly, many of those that can't reach their favoured universities will repeat the exam 2 or 3 times. As such, exam first-timers are often competing against people that have dedicated 3 years to the exam which makes their prospects even worse. Even amongst those that graduate from the top universities, a large number will choose to sit the biannual public administration exam. Some of these exams have an admission rate less than Harvard which is why many people typically devote years to the task despite no guarantee of success. On the other end of the spectrum there are the 3D jobs (Dirty, Dangerous and Difficult). The unofficial fourth 'D' is 'demeaning'. 3D jobs aren't inherently low paid or 'demeaning'. But in a conformist society where the be-all and end-all is academic rigour then those that refuse to play are considered losers. Sandel spends a lot of time comparing the education system to a merit sorting machine. Sorting and ranking is ultimately a mathematical discipline. Take the feature vector [x1,x2...xn] and the weighting vector [w1,w2,...wn], calculate the inner product and now you have your single number for ranking. In this analogy the features would be a student's abilities and the weight vector would be how each ability is valued in society. If there are a variety of weighting vectors then each student has several options of optimising their own feature vector. But if the weight vector is always [1,0,...0] then students can only compete on the x1 feature. Even if there are other weight vectors around they have to be made known when students start optimising for their futures. What ends up being measured is a students' ability to optimise i.e. study for a test. There is a correlation between test marks and good characteristics such as intelligence and hard work. But there is an even higher correlation for parents income, which makes test scores as a measure of ability entirely unfair.","title":"Meritocracy"},{"location":"meritocracy/#meritocracy","text":"I recently read Michael Sandel's book \"The Tyranny of Merit\" on the recommendation of The Guardian's best books of 2020. The book is good and although it is mostly focused on the American implementation of a meritocratic system it was enough to reveal the inconsistent values that I see in Australia and to an even worse extent Korea. Truthfully, I am a benefactor of the meritocratic systems of the countries that I have lived in. I've received an expensive education. I work in an economy where my talents and intellectual interests are highly valued, which gives me the double benefit of status and wealth. But I could have just as easily been born in a situation where I wasn't able to receive an expensive education or live in an economy where my labour is valued. And neither case would be the result of my choices. While reading I felt the most pity for the current generation of Koreans that are in or about to enter their 20s. South Korea, as Sandel noted, might be the most meritocratic country in the world. The university admission exams are conducted yearly, many of those that can't reach their favoured universities will repeat the exam 2 or 3 times. As such, exam first-timers are often competing against people that have dedicated 3 years to the exam which makes their prospects even worse. Even amongst those that graduate from the top universities, a large number will choose to sit the biannual public administration exam. Some of these exams have an admission rate less than Harvard which is why many people typically devote years to the task despite no guarantee of success. On the other end of the spectrum there are the 3D jobs (Dirty, Dangerous and Difficult). The unofficial fourth 'D' is 'demeaning'. 3D jobs aren't inherently low paid or 'demeaning'. But in a conformist society where the be-all and end-all is academic rigour then those that refuse to play are considered losers. Sandel spends a lot of time comparing the education system to a merit sorting machine. Sorting and ranking is ultimately a mathematical discipline. Take the feature vector [x1,x2...xn] and the weighting vector [w1,w2,...wn], calculate the inner product and now you have your single number for ranking. In this analogy the features would be a student's abilities and the weight vector would be how each ability is valued in society. If there are a variety of weighting vectors then each student has several options of optimising their own feature vector. But if the weight vector is always [1,0,...0] then students can only compete on the x1 feature. Even if there are other weight vectors around they have to be made known when students start optimising for their futures. What ends up being measured is a students' ability to optimise i.e. study for a test. There is a correlation between test marks and good characteristics such as intelligence and hard work. But there is an even higher correlation for parents income, which makes test scores as a measure of ability entirely unfair.","title":"Meritocracy"},{"location":"python_profiling/","text":"Python Profiling I've seen a few posts on Hacker News recently about writing fast and efficient code. Fortunately this example was written in Python which is probably the language that could save the most instruction cycles globally with a bit of optimization. I say this because Python is ubiquitous when it comes to writing prototype scripts that eventually get pushed to production under project deadlines. The rule of thumb that I use, and remind my coworkers is that the order of code optimization is: Make it work - optimize for business functionality (MVP) Make it right - optimize for human readability (documentation, test cases, extendable) Make it fast - optimize for execution speed In the work of data science, the opportunities to get to the third stage are rare enough that they aren't covered in many learning materials but important enough that you need to know them. In this post i'll cover the profiling basics when using Python. Be aware that many profiling results are dependent on the current state of the computer running the profiler, it's best to run each result several times to normalize across background processes that may be happening. %timeit For those using Jupyter notebooks or an iPython environment, %timeit is the easiest way to start measuring execution time of functions and therefore being able to compare speeds of different solutions. timeit % timeit [ i ** 3 for i in range ( 100 )] #100000 loops, best of 5: 12.1 \u00b5s per loop %memit memit is the easiest way to start memory profiling in a Jupyter Notebook or iPython environment. memit ! pip install memory_profiler % load_ext memory_profiler % memit ( i ** 3 for i in range ( 10000 )) % memit [ i ** 3 for i in range ( 10000 )] /usr/bin/time /usr/bin/time is a unix utility to time the execution of any program. It doesn't have as much detail as the python specific profilers but can be used for all types of programs that run as a unix CLI. /usr/bin/time curl https://matthewburke.xyz >> /dev/null cProfile cProfile is part of the standard library and will show you the time spent in each function of a certain program. python -m cProfile -s cumulative script.py python -m cProfile -s cumulative script.py | grep script.py line_profiler Once you've worked out which function is using up the most CPU resources you can dive in deeper and investigate which line is using the most CPU resources with line_profiler. To run this you'll need to download the pip package line_profiler, add the @profile decorator to the functions you want to profile and finally run the line_profiler from the command line ``` py title=\"function_to_profile.py @profile def slow_add(a: int, b: int) -> int: time.sleep(1) return a+b ``` bash kernprof -l function_to_profile.py See the github repo github repo for more information. memory_profiler The memory_profiler works in much the same way as the line_profiler. Have a look at the official documentation to get started. py-spy py-spy (note the hyphen) is an incredible piece of work which allows you to profile a python process as it is running without slowing down the process (too much). You can imagine how useful it would be when trying to determine what the bottleneck is for a web server when serving real traffic. You can find more information on the github repo .","title":"Python Profiling"},{"location":"python_profiling/#python-profiling","text":"I've seen a few posts on Hacker News recently about writing fast and efficient code. Fortunately this example was written in Python which is probably the language that could save the most instruction cycles globally with a bit of optimization. I say this because Python is ubiquitous when it comes to writing prototype scripts that eventually get pushed to production under project deadlines. The rule of thumb that I use, and remind my coworkers is that the order of code optimization is: Make it work - optimize for business functionality (MVP) Make it right - optimize for human readability (documentation, test cases, extendable) Make it fast - optimize for execution speed In the work of data science, the opportunities to get to the third stage are rare enough that they aren't covered in many learning materials but important enough that you need to know them. In this post i'll cover the profiling basics when using Python. Be aware that many profiling results are dependent on the current state of the computer running the profiler, it's best to run each result several times to normalize across background processes that may be happening.","title":"Python Profiling"},{"location":"python_profiling/#timeit","text":"For those using Jupyter notebooks or an iPython environment, %timeit is the easiest way to start measuring execution time of functions and therefore being able to compare speeds of different solutions. timeit % timeit [ i ** 3 for i in range ( 100 )] #100000 loops, best of 5: 12.1 \u00b5s per loop","title":"%timeit"},{"location":"python_profiling/#memit","text":"memit is the easiest way to start memory profiling in a Jupyter Notebook or iPython environment. memit ! pip install memory_profiler % load_ext memory_profiler % memit ( i ** 3 for i in range ( 10000 )) % memit [ i ** 3 for i in range ( 10000 )]","title":"%memit"},{"location":"python_profiling/#usrbintime","text":"/usr/bin/time is a unix utility to time the execution of any program. It doesn't have as much detail as the python specific profilers but can be used for all types of programs that run as a unix CLI. /usr/bin/time curl https://matthewburke.xyz >> /dev/null","title":"/usr/bin/time"},{"location":"python_profiling/#cprofile","text":"cProfile is part of the standard library and will show you the time spent in each function of a certain program. python -m cProfile -s cumulative script.py python -m cProfile -s cumulative script.py | grep script.py","title":"cProfile"},{"location":"python_profiling/#line_profiler","text":"Once you've worked out which function is using up the most CPU resources you can dive in deeper and investigate which line is using the most CPU resources with line_profiler. To run this you'll need to download the pip package line_profiler, add the @profile decorator to the functions you want to profile and finally run the line_profiler from the command line ``` py title=\"function_to_profile.py @profile def slow_add(a: int, b: int) -> int: time.sleep(1) return a+b ``` bash kernprof -l function_to_profile.py See the github repo github repo for more information.","title":"line_profiler"},{"location":"python_profiling/#memory_profiler","text":"The memory_profiler works in much the same way as the line_profiler. Have a look at the official documentation to get started.","title":"memory_profiler"},{"location":"python_profiling/#py-spy","text":"py-spy (note the hyphen) is an incredible piece of work which allows you to profile a python process as it is running without slowing down the process (too much). You can imagine how useful it would be when trying to determine what the bottleneck is for a web server when serving real traffic. You can find more information on the github repo .","title":"py-spy"}]}